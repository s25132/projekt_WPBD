# Projekt WPBD - System Strumieniowego Przetwarzania Danych

## ğŸ“‹ Opis Projektu

Projekt WPBD to kompleksowy system do strumieniowego przetwarzania danych, ktÃ³ry implementuje nowoczesnÄ… architekturÄ™ big data. System automatycznie synchronizuje zmiany z bazy danych PostgreSQL do Apache Kafka, a nastÄ™pnie przetwarza je w czasie rzeczywistym za pomocÄ… Apache Spark i zapisuje w formacie Delta Lake w MinIO (S3).

## ğŸ—ï¸ Architektura Systemu

System skÅ‚ada siÄ™ z nastÄ™pujÄ…cych komponentÃ³w:

### Warstwa Danych
- **PostgreSQL 16** - relacyjna baza danych z wÅ‚Ä…czonÄ… replikacjÄ… logicznÄ…
- **MinIO** - obiektowe przechowywanie danych kompatybilne z S3

### Warstwa Strumieniowa
- **Apache Kafka (KRaft)** - broker wiadomoÅ›ci do przesyÅ‚ania zdarzeÅ„
- **Debezium Connect** - Change Data Capture (CDC) dla PostgreSQL
- **Kafka UI** - interfejs webowy do zarzÄ…dzania Kafka

### Warstwa Przetwarzania
- **Apache Spark 3.5.1** - przetwarzanie strumieniowe danych
  - Spark Master - koordynator klastra
  - Spark Worker - wÄ™zeÅ‚ wykonawczy
  - Spark Submit (orders & users) - aplikacje strumieniowe

### UsÅ‚ugi Pomocnicze
- **Connector Configurer** - automatyczna konfiguracja konektorÃ³w Debezium
- **Seeder** - inicjalizacja danych testowych
- **Seeder Cyclic** - ciÄ…gÅ‚e generowanie nowych danych (co 30s)

## ğŸš€ Co moÅ¼esz zrobiÄ‡?

### 1. Uruchomienie CaÅ‚ego Systemu

```bash
# Uruchom wszystkie usÅ‚ugi
docker-compose up -d

# SprawdÅº status usÅ‚ug
docker-compose ps

# Zobacz logi wszystkich usÅ‚ug
docker-compose logs -f

# Zobacz logi konkretnej usÅ‚ugi
docker-compose logs -f kafka
docker-compose logs -f spark-submit-orders
```

### 2. Monitorowanie i ZarzÄ…dzanie

#### Kafka UI
- **URL**: http://localhost:8081
- **Funkcje**: PrzeglÄ…danie tematÃ³w, wiadomoÅ›ci, konsumentÃ³w

#### Spark Master UI
- **URL**: http://localhost:8082
- **Funkcje**: Monitoring zadaÅ„ Spark, workery, aplikacje

#### Spark Worker UI
- **URL**: http://localhost:8013
- **Funkcje**: Status wykonywanych zadaÅ„

#### MinIO Console
- **URL**: http://localhost:9001
- **Login**: minioadmin / minioadmin
- **Funkcje**: PrzeglÄ…danie bucketÃ³w i zapisanych danych

### 3. Praca z BazÄ… Danych

```bash
# PoÅ‚Ä…cz siÄ™ z PostgreSQL
docker exec -it postgres16 psql -U app -d appdb

# PrzykÅ‚adowe zapytania SQL
SELECT COUNT(*) FROM users;
SELECT COUNT(*) FROM orders;
SELECT * FROM users LIMIT 10;
SELECT o.*, u.full_name FROM orders o JOIN users u ON o.user_id = u.id LIMIT 10;

# Dodaj nowego uÅ¼ytkownika (automatycznie trafi do Kafka!)
INSERT INTO users (full_name, email, created_at) 
VALUES ('Jan Kowalski', 'jan@example.com', NOW());
```

### 4. Sprawdzanie Danych w Kafka

```bash
# Lista tematÃ³w
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# PodglÄ…d wiadomoÅ›ci z tematu users
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic appdb.public.users \
  --from-beginning \
  --max-messages 5

# PodglÄ…d wiadomoÅ›ci z tematu orders
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic appdb.public.orders \
  --from-beginning \
  --max-messages 5
```

### 5. Praca z MinIO i Danymi w Delta Lake

```bash
# PoÅ‚Ä…cz siÄ™ z MinIO Client
docker exec -it mc sh

# WyÅ›wietl buckety
mc ls minio/

# WyÅ›wietl zawartoÅ›Ä‡ datalake
mc ls minio/datalake/
mc ls minio/datalake/tables/
mc ls minio/datalake/tables/orders/
mc ls minio/datalake/tables/users/

# Pobierz plik Parquet lokalnie (opcjonalnie)
mc cp minio/datalake/tables/orders/part-00000-*.parquet /tmp/
```

### 6. Testowanie CDC (Change Data Capture)

```bash
# 1. Dodaj nowego uÅ¼ytkownika
docker exec postgres16 psql -U app -d appdb -c \
  "INSERT INTO users (full_name, email, created_at) VALUES ('Test User', 'test@test.com', NOW());"

# 2. SprawdÅº w Kafka UI (http://localhost:8081)
#    - Zobacz temat: appdb.public.users
#    - PowinieneÅ› zobaczyÄ‡ nowÄ… wiadomoÅ›Ä‡ z operacjÄ… "c" (create)

# 3. Zaktualizuj uÅ¼ytkownika
docker exec postgres16 psql -U app -d appdb -c \
  "UPDATE users SET full_name = 'Test User Updated' WHERE email = 'test@test.com';"

# 4. W Kafka pojawi siÄ™ wiadomoÅ›Ä‡ z operacjÄ… "u" (update)

# 5. UsuÅ„ uÅ¼ytkownika
docker exec postgres16 psql -U app -d appdb -c \
  "DELETE FROM users WHERE email = 'test@test.com';"

# 6. W Kafka pojawi siÄ™ wiadomoÅ›Ä‡ z operacjÄ… "d" (delete)
```

### 7. ZarzÄ…dzanie Konektorami Debezium

```bash
# Lista konektorÃ³w
curl http://localhost:8083/connectors

# Status konkretnego konektora
curl http://localhost:8083/connectors/postgres-source/status | jq

# Restart konektora
curl -X POST http://localhost:8083/connectors/postgres-source/restart

# UsuniÄ™cie konektora
curl -X DELETE http://localhost:8083/connectors/postgres-source
```

### 8. Zatrzymanie i Czyszczenie

```bash
# Zatrzymaj wszystkie usÅ‚ugi
docker-compose down

# Zatrzymaj i usuÅ„ wolumeny (UWAGA: usuwa wszystkie dane!)
docker-compose down -v

# Zatrzymaj konkretnÄ… usÅ‚ugÄ™
docker-compose stop spark-submit-orders
```

### 9. Debugowanie i RozwiÄ…zywanie ProblemÃ³w

```bash
# SprawdÅº logi konkretnej usÅ‚ugi
docker-compose logs connector_configurer
docker-compose logs seeder
docker-compose logs spark-submit-orders

# WejdÅº do kontenera
docker exec -it postgres16 bash
docker exec -it kafka bash
docker exec -it spark-master bash

# SprawdÅº uÅ¼ycie zasobÃ³w
docker stats

# Zrestartuj konkretnÄ… usÅ‚ugÄ™
docker-compose restart kafka
```

### 10. Modyfikacja Konfiguracji

#### Zmiana liczby generowanych rekordÃ³w
```bash
# Edytuj docker-compose.yml
# ZnajdÅº sekcjÄ™ 'seeder' i zmieÅ„ zmiennÄ… ROWS
ROWS: "500"  # zamiast domyÅ›lnych 200
```

#### Zmiana czÄ™stotliwoÅ›ci cyklicznego seedowania
```bash
# W docker-compose.yml, sekcja 'seeder-cyclic'
# ZmieÅ„ wartoÅ›Ä‡ sleep (domyÅ›lnie 30 sekund)
sleep 60;  # bÄ™dzie dodawaÄ‡ dane co minutÄ™
```

## ğŸ“Š PrzepÅ‚yw Danych

```
PostgreSQL â†’ Debezium â†’ Kafka â†’ Spark Streaming â†’ Delta Lake (MinIO)
    â†“                      â†“                            â†“
  Tables              Topics: orders,              Parquet Files
  (users,             users                        in S3-compatible
  orders)                                          storage
```

1. **Seeder** tworzy tabele i wypeÅ‚nia je danymi testowymi
2. **Seeder Cyclic** co 30s dodaje nowe rekordy
3. **Debezium** przechwytuje zmiany w PostgreSQL (CDC)
4. **Kafka** przechowuje zdarzenia w tematach
5. **Spark** czyta strumienie z Kafka w czasie rzeczywistym
6. **Delta Lake** zapisuje dane w formacie Parquet w MinIO

## ğŸ”§ Wymagania Systemowe

- Docker Desktop lub Docker Engine + Docker Compose
- Minimum 8GB RAM (zalecane 16GB)
- Minimum 10GB wolnego miejsca na dysku

## ğŸ“ Struktura PlikÃ³w

```
.
â”œâ”€â”€ docker-compose.yml          # Definicja wszystkich usÅ‚ug
â”œâ”€â”€ connector_configurer/       # Konfiguracja Debezium
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ pg-source.json
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ seeder/                     # Generowanie danych testowych
â”‚   â”œâ”€â”€ seed.py
â”‚   â”œâ”€â”€ cyclic_job.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ spark/                      # Aplikacje Spark Streaming
â”‚   â”œâ”€â”€ read_orders.py
â”‚   â””â”€â”€ read_users.py
â””â”€â”€ README.md
```

## ğŸ› RozwiÄ…zywanie ProblemÃ³w

### Spark nie moÅ¼e siÄ™ poÅ‚Ä…czyÄ‡ z Kafka
- SprawdÅº czy Kafka jest uruchomiona: `docker-compose ps kafka`
- Zobacz logi: `docker-compose logs kafka`

### Debezium Connector nie dziaÅ‚a
- SprawdÅº status: `curl http://localhost:8083/connectors/postgres-source/status`
- SprawdÅº logi: `docker-compose logs connect`

### Brak danych w MinIO
- SprawdÅº czy Spark job dziaÅ‚a: `docker-compose logs spark-submit-orders`
- SprawdÅº Spark UI: http://localhost:8082

### PostgreSQL nie przyjmuje poÅ‚Ä…czeÅ„
- SprawdÅº healthcheck: `docker-compose ps db`
- SprawdÅº logi: `docker-compose logs db`

## ğŸ“š Dodatkowe Zasoby

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Debezium Documentation](https://debezium.io/documentation/)
- [Apache Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Delta Lake Documentation](https://docs.delta.io/)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)

## ğŸ¯ MoÅ¼liwe Rozszerzenia

1. **Dodanie nowych tabel** - rozszerz seeder i stwÃ³rz nowe Spark jobs
2. **Integracja z BI tools** - poÅ‚Ä…cz siÄ™ z Tableau/Power BI do MinIO
3. **Machine Learning** - uÅ¼yj MLlib w Spark do analizy predykcyjnej
4. **Alerting** - dodaj monitoring i alerty dla anomalii w danych
5. **API Layer** - stwÃ³rz REST API do odpytywania Delta Lake
6. **Data Quality** - dodaj walidacjÄ™ jakoÅ›ci danych w Spark
7. **Partitioning** - zoptymalizuj Delta Lake przez partycjonowanie
8. **Time Travel** - wykorzystaj moÅ¼liwoÅ›ci wersjonowania Delta Lake

## ğŸ“„ Licencja

Projekt edukacyjny - WPBD (WyÅ¼sza Projektowanie Baz Danych)
